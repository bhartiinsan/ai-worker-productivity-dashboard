# ğŸ—ï¸ Architecture Deep Dive

## Data Journey: Edge Device â†’ Real-Time Insights

The system implements a **four-stage data pipeline** for deterministic event processing and metric derivation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: EDGE DEVICE (CCTV AI Ingestion)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Location: On-premise CCTV cameras + local AI inference             â”‚
â”‚ Process:                                                             â”‚
â”‚  â€¢ Real-time video analysis via pre-trained ML model (e.g., YOLOv8) â”‚
â”‚  â€¢ Event classification: working / idle / absent / product_count     â”‚
â”‚  â€¢ Confidence scoring (0.0â€“1.0)                                      â”‚
â”‚  â€¢ Local SQLite buffer for network resilience (store-and-forward)   â”‚
â”‚  â€¢ Batch assembly when connectivity restored or buffer fills         â”‚
â”‚ Output: JSON event array to API                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ HTTPS POST
                                   â–¼ (Batch: 1â€“1000 events)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: API INGESTION & DEDUPLICATION                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer: FastAPI (1 instance) + Rate Limiter (100 req/min)           â”‚
â”‚ Deduplication Strategy:                                              â”‚
â”‚  â€¢ Unique key: (timestamp, worker_id, event_type)                   â”‚
â”‚  â€¢ Logic: If (ts, worker, event) seen before â†’ skip                 â”‚
â”‚  â€¢ Handles out-of-order arrival via SQL UNIQUE INDEX               â”‚
â”‚ Validation:                                                           â”‚
â”‚  â€¢ Worker & workstation existence check                              â”‚
â”‚  â€¢ Confidence threshold enforcement (â‰¥ 0.7)                         â”‚
â”‚  â€¢ Pydantic schema validation (ISO 8601 timestamps)                â”‚
â”‚ Response: Ingestion report (success, duplicate, error counts)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ INSERT/IGNORE
                                   â–¼ (Time-indexed)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: DATABASE PERSISTENCE (Bitemporal Tracking)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Store: SQLAlchemy ORM + SQLite (production: PostgreSQL)            â”‚
â”‚ Tables:                                                               â”‚
â”‚  â€¢ AIEvents: (id, timestamp, worker_id, workstation_id,             â”‚
â”‚              event_type, confidence, count, created_at, updated_at) â”‚
â”‚  â€¢ Workers: (id, name, location, active_since)                      â”‚
â”‚  â€¢ Workstations: (id, name, location, line, capacity)              â”‚
â”‚ Bitemporal Approach:                                                â”‚
â”‚  â€¢ event_time (timestamp): When activity occurred (per CCTV)       â”‚
â”‚  â€¢ created_at: Server insertion time                                â”‚
â”‚  â€¢ Enables audit trail and historical reconstruction                â”‚
â”‚ Indexing:                                                            â”‚
â”‚  â€¢ Clustered on (worker_id, timestamp) for metric queries           â”‚
â”‚  â€¢ Separate index on created_at for audit/compliance                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ SELECT queries
                                   â–¼ (Chronological aggregation)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: REAL-TIME METRIC AGGREGATION                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ On-demand Computation (React query â†’ FastAPI â†’ aggregation)         â”‚
â”‚ Worker Metrics:                                                      â”‚
â”‚  â€¢ Utilization = (working_hours / elapsed_hours) Ã— 100%             â”‚
â”‚  â€¢ Throughput = total_units_produced / working_hours                â”‚
â”‚  â€¢ Availability = (1 - absent_hours / elapsed_hours) Ã— 100%         â”‚
â”‚ Workstation Metrics:                                                 â”‚
â”‚  â€¢ Occupancy = sum(worker_present) / time_window                    â”‚
â”‚  â€¢ Efficiency = units_produced / occupancy_hours                    â”‚
â”‚ Factory Metrics:                                                     â”‚
â”‚  â€¢ Overall Utilization: Weighted by worker count                    â”‚
â”‚  â€¢ Production Target Variance: Actual vs. baseline                   â”‚
â”‚  â€¢ Shift Handover Analysis: Productivity dips (e.g., 10:00â€“10:15)   â”‚
â”‚ Caching Strategy:                                                    â”‚
â”‚  â€¢ In-memory cache (60s TTL) for dashboard refreshes                â”‚
â”‚  â€¢ Re-compute on new event ingestion                                 â”‚
â”‚ Output: JSON KPI objects to React Dashboard                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ REST (JSON)
                                   â–¼ (React Query)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND PRESENTATION (React + TypeScript + Tailwind)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Factory KPI cards: Workers active, avg. utilization, production   â”‚
â”‚ â€¢ Leaderboard: Top 3 workers by output (real-time update)          â”‚
â”‚ â€¢ Station grid: Utilization heatmap (red: idle, green: working)    â”‚
â”‚ â€¢ Event stream: Chronological AI event log with badges              â”‚
â”‚ â€¢ Charts: Productivity trend (hourly/daily, Recharts)              â”‚
â”‚ â€¢ Dark mode + animations (Framer Motion)                            â”‚
â”‚ â€¢ Responsive: Mobile, tablet, desktop layouts                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Design Principles

### 1. Determinism
Timestamp-based ordering ensures consistent aggregation across replays. Events are sorted chronologically before metric calculation, guaranteeing reproducible results.

### 2. Resilience
Local buffering + deduplication survive connectivity drops. Edge devices cache up to 10,000 events locally and use exponential backoff retry strategy.

### 3. Auditability
Bitemporal tracking preserves "what was known when" for compliance. Both event_time and created_at are stored, enabling reconstruction of historical states.

### 4. Scalability
Metric computation is read-optimized; indexing scales to 100M+ events. Database queries use covering indexes for sub-100ms response times.

---

## Scaling to 100+ Sites

**Current Architecture (6 Cameras):**
- SQLite database
- Single FastAPI instance
- Direct camera â†’ backend connection

**Scaled Architecture (100+ Cameras):**

```
Edge (100+ Cameras)
        â†“
    Nginx (Load Balancer + Rate Limiting)
        â†“
FastAPI Cluster (5-20 pods in Kubernetes)
        â†“
    Apache Kafka (Event Stream Buffer)
        â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â†“      â†“      â†“          â†“
PostgreSQL Redis ClickHouse TimescaleDB
(OLTP)   (Cache) (Analytics) (Time-Series)
```

### Key Architectural Changes

**1. Database Migration: SQLite â†’ PostgreSQL + TimescaleDB**
```sql
-- Partition by month for fast queries
CREATE TABLE ai_events_2026_01 
PARTITION OF ai_events
FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');

-- Hypertable for time-series (TimescaleDB)
SELECT create_hypertable('ai_events', 'timestamp');

-- Indexes for common queries
CREATE INDEX idx_worker_time ON ai_events(worker_id, timestamp DESC);
CREATE INDEX idx_workstation_time ON ai_events(workstation_id, timestamp DESC);
```

**2. Event Streaming: Kafka for Decoupling**
```python
# FastAPI Producer (handles 1000s of req/sec)
@app.post("/api/events")
async def ingest_event(event: AIEventCreate):
    await kafka_producer.send('ai-events', event.dict())
    return {"status": "queued"}
```

**3. Distributed Caching: Redis**
- Cache worker/workstation metrics for 60 seconds
- Invalidate on new event ingestion
- Reduces database load by 90%

**4. Read Replicas**
- Primary PostgreSQL for writes
- 3 read replicas for metric queries
- Load balanced across replicas

---

## Network Resilience

### Store-and-Forward Mechanism

Edge devices implement local buffering when network connectivity is lost:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Edge Device (CCTV AI)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Generate event           â”‚
â”‚ 2. Try POST to backend      â”‚
â”‚ 3. If network fails:        â”‚
â”‚    â€¢ Store in local SQLite  â”‚
â”‚    â€¢ Retry: 1s, 2s, 4s...   â”‚
â”‚ 4. On reconnect:            â”‚
â”‚    â€¢ Batch upload queue     â”‚
â”‚    â€¢ Use /api/events/batch  â”‚
â”‚ 5. Backend deduplicates     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Technical Details:**
- **Edge Storage**: SQLite buffer on edge device (persists across reboots)
- **Retry Strategy**: Exponential backoff: 1s â†’ 2s â†’ 4s â†’ 8s â†’ ... â†’ 5min max
- **Batch Uploads**: 100 events/request when connectivity restored
- **Bounded Buffer**: Ring buffer keeps last 10,000 events to prevent memory overflow
- **Health Monitoring**: Edge pings backend `/health` every 30s to detect connectivity

**Production Example:**
```
10:00 AM - Network drops
10:00-10:15 - Edge buffers 150 events locally
10:15 AM - Connection restored
10:15:05 - Batch upload #1: Events 1-100
10:15:12 - Batch upload #2: Events 101-150
Dashboard reflects data with 15-minute lag visible in bitemporal tracking
```

---

## Model Drift Detection

The system monitors AI model confidence scores to detect degradation over time:

**Monitoring Strategy:**

1. **Real-Time Metrics**
   ```http
   GET /api/metrics/model-health
   
   Response:
   {
     "status": "Healthy",
     "avg_confidence": 0.9245,
     "samples": 100,
     "message": "Model confidence is within acceptable range"
   }
   ```

2. **Dashboard Visualization**
   - Line chart: 7-day rolling average confidence
   - Heatmap: Confidence by camera/workstation
   - Color codes:
     - Red < 0.75 (Critical - Retrain)
     - Yellow 0.75-0.85 (Warning - Monitor)
     - Green > 0.85 (Healthy)

3. **Automated Alerting**
   - Trigger alert if average confidence drops below 0.75
   - Alert on sudden confidence drop >10% in 1 hour
   - Weekly model performance reports

---

## Security Architecture

### Authentication & Authorization
- API key authentication for event ingestion
- Rate limiting: 100 requests/minute per IP
- CORS configured for specific frontend origins

### Data Protection
- HTTPS enforced in production
- Environment variables for sensitive config
- Database credentials in .env (not committed)

### Monitoring & Logging
- Structured logging with log levels (DEBUG, INFO, WARNING, ERROR)
- Health check endpoint for uptime monitoring
- Request/response logging for audit trails

---

## Performance Optimizations

1. **Database Indexes**
   - Composite index on (worker_id, timestamp)
   - Separate indexes on workstation_id and event_type
   - Covering indexes for common query patterns

2. **Query Optimization**
   - Use of SQLAlchemy's lazy loading
   - Batch queries instead of N+1 patterns
   - Limit result sets with pagination

3. **Caching Strategy**
   - In-memory cache for frequently accessed metrics
   - 60-second TTL for dashboard data
   - Cache invalidation on new event ingestion

4. **Frontend Optimizations**
   - Component memoization with React.memo
   - Debounced auto-refresh (30 seconds)
   - Virtual scrolling for large event lists

---

## Deployment Architecture

### Docker Compose (Current)
```yaml
services:
  backend:
    - FastAPI + Uvicorn
    - Port 8000
    - SQLite volume mount
  
  frontend:
    - React + Nginx
    - Port 3000 (development) / 80 (production)
    - Proxy to backend API
```

### Kubernetes (Production)
```yaml
Deployments:
  - backend-deployment (3 replicas)
  - frontend-deployment (2 replicas)
  - postgres-statefulset (1 primary, 2 replicas)

Services:
  - backend-service (ClusterIP)
  - frontend-service (LoadBalancer)
  - postgres-service (Headless)

Ingress:
  - TLS termination
  - Path-based routing (/api â†’ backend, / â†’ frontend)
```

---

## Future Enhancements

1. **WebSocket Support**
   - Real-time event streaming to dashboard
   - Sub-second metric updates
   - Live worker status changes

2. **Advanced Analytics**
   - Predictive maintenance alerts
   - Anomaly detection with ML
   - Shift optimization recommendations

3. **Multi-tenancy**
   - Support for multiple factories
   - Tenant isolation
   - Role-based access control

4. **Mobile App**
   - iOS/Android native apps
   - Push notifications for alerts
   - Offline mode with sync
